
Real-time DRX tuning change procedure
-------------------------------------
OLD, see below for new version
slot s-2: Receive DRX command from MCS
slot s-1: Process DRX command
            Send command to server pipelines:
              Metadata includes new UTC_START and tuning parameters
            Restart roach flow:
              sleep 0.5
              stop roach flow on next PPS (tx_enable(0x0))
              sleep 1.0
              change roach tuning registers
              start roach flow on next PPS (tx_enable(0x3))
slot s  : Command is applied (new settings come into effect)

ACTUALLY, break into separate 'STP' and 'DRX' commands:
  slot s-2: Receive STP-DRX command
  slot s-1: Process STP-DRX command: sleep 0.5, tx_enable(0x0)
            Receive DRX command
  slot s:   (Roach flow stops)
            Process DRX command: sleep 0.5, change registers, tx_enable(0x3)
                                 Send command to server pipelines
  slot s+1: (Roach flow starts again with new settings)

Pipeline flow management:
  Task receives STOP command before (or perhaps while) input.open/advance()
  Task breaks out of processing loop and calls input.close()
  Task receives START command plus metadata
  Task calls input.open() and enters processing loop
  Task receives STOP command
  Task receives SHUTDOWN command
  Task breaks out of main loop and its thread ends

** ZMQ tcp sockets for commands?
     Allow inter-task comms and/or remote control
     Which socket type?

Reorder task:
  Receives START/STOP/SHUTDOWN commands from remote adp-control service
    New UTC_START, channel info etc.
  Passes them on to its data output

Correlator task:
  Receives updates to COR_NAVG, DRX_TUNING_MASK and COR_GAIN
  Have accompanying 'time', 'navg' and 'gain' outputs alongside visibility snapshots
  Updates arriving mid-integration cause it to be cancelled

TBF task:

FileRead:
  while running:
    _file.open(filename);
	//output.push("event:start, filename:"+filename);
	output.start(header_str);
    while processing:
	  auto out = output.open();
	  _file.read(out, out.size_bytes());
	  if( !_file ) {
	    out.set_nframe(_file.gcount());
	    //out.set_eod();
		break;
	  }
	//output.push(
	output.finish();

Constructor sets input and output names and output spaces

RingBuffer3:
  Need separate 'queue' objects for state management (and headers etc.)
  Treat actual ring buffer as low-level object whose state is not to be
    used at a high level.
	I.e., _head etc. only used indirectly

Flow:
  _start;      // Offset in underlying RingBuffer
  //_finish;     // Offset in underlying RingBuffer
  _head;       // Offset relative to _start and _finish
  _tail;       // Offset relative to _start and _finish
  Ring* _data; // Underlying data storage
  //string _hdr; // Header metadata storage

Pipe:
  bool        shutdown = false;
  Ring        data_ring;
  Ring<Flow>  flow_ring;
  //queue<Flow> flows;
  WriteFlow start_flow();
  ReadFlow  open_flow();

Pipe.start_flow(hdr):
  flow_ring.open_write();
  write start = data_ring._reserve_head;
  write head  = 0;
  write tail  = 0;
  write hdr   = hdr;
  return


PipeWriter : public RingWriter<Ring<Flow> >
PipeReader : public RingReader<Ring<Flow> >
FlowWriter : public RingWriter<Ring<data> >
FlowReader : public RingReader<Ring<data> >

Task:
  PipeReader input1;
  PipeWriter output1;

FlowWriter     out_flow  = task.output1.start_flow(hdr);
RingWriteBlock out_block = out_flow.open_block();

FlowReader in_flow = task.input1.open_flow();
in_flow.open(guaranteed);

Task:
  map<string,Pipe> _pipes;

while true:
  try:
    in_flow1 = input1.open_flow();
    in_flow2 = input2.open_flow();
  catch PipeShutdown:
    break;
  try:
    in_flow3 = input3.open_flow(); // Optional inputs
  catch PipelineShutdown:
    pass
  // Process input headers into output headers...
  out_flow1 = output1.start_flow(out_hdr1);
  out_flow2 = output2.start_flow(out_hdr2);
  in_flow1.open(guaranteed);
  in_flow2.open(guaranteed);
  do:
    out_flow1.open_block();
	out_flow2.open_block();
	process();
	try:
	  input1.advance();
	  input2.advance();
	except EOD: // EOD with no further data available (i.e., EOD was set _before_ advancing)
	  break;
	out_flow1.close_block();
	out_flow2.close_block();
  while true;

while inputs_alive:
  in_hdrs = wait_for_inputs_start();
  start_outputs(out_hdrs);
  open_inputs();
  while inputs_data_available:
    open_output_blocks();
	process();
	close_output_blocks();
    advance_inputs();
  close_inputs();
  finish_outputs();

start(hdr)
open_write(

  States:
    start(hdr): active = true, reset head etc.
	open_write()
	close_write()
	open_read_at()
	close_read()
	finish(): active = false

FileWrite:
  for each input:
    input.set_ring(...);
  for each output:
    output.set_ring(...);
  while running:
    // TODO: Is there an issue with headers being overwritten after a very short observation?
    in_headers['input1'] = input1.wait_for_start();
	in_headers['input2'] = input2.wait_for_start();
	// Automatically set RingReader shapes to match 'shape' in the headers
	input1.set_shape(in_headers['input1']['shape']);
	input2.set_shape(in_headers['input2']['shape']);
	out_headers = Object();
	this->start(in_headers, out_headers);
	  // User overrides read shape with custom value
	  input2.set_shape(...);
	  // Headers typically copied to output
	  out_headers = in_headers;
	  // User specifies output shapes (which otherwise default to 1)
	  out_headers['output1']['shape'] = in_headers['input1']['shape'][:-1];
	  out_headers['output2']['shape'] = ...;
	  //output1.set_shape(in_headers['input1']['shape'][:-1];
	  //output2.set_shape(...);
	  _file.open(in_headers['input1']['filename']);
	// Set output shapes to default to 1 if not specified by user
	if 'shape' not in out_headers['output1']:
	  out_headers['output1']['shape'] = 1
	...
	// Automatically set RingWriter shapes to match 'shape' in the headers
	output1.set_shape(out_headers['output1']['shape']);
	output2.set_shape(out_headers['output2']['shape']);
	// Make ring memory allocation requests
	// TODO: Allow user to call these themselves if required (in start() or separate callback?)
	for each input:
	  input.request_size_bytes(...);
	for each output:
	  output.request_size_bytes(...);
	// Call start on all outputs
	for each output:
	  output.start(out_headers[output]);
	// Open inputs for reading
	for each input:
	  input.open(guaranteed);
	// TODO: End-of-data support
	while processing:
	  this->process();
	    _file.write(&input[0], input.size_bytes());
	  // Check if EOD occurred within the currently-open span
	  if input[0].eod(): // TODO: Just primary input?
	    break;
	  this->advance();
	// Call finish on all outputs
	this->finish();
	  _file.close();
	for each output:
	  output.finish();
	for each input:
	  input.close();
	
    
    size_t header_size;
    header_str = input.start(&header_size);
	Object header_obj = parse_value(header_str, header_str+header_size).get<Object>();
	this->start(header_obj);
	  _file.open(lookup_string(header_obj, 'filename'));
	  input.request_size 
    input.open();
    

** TODO: Output spaces must be set before calling request_*
           To support input dependency, must set space in the constructor
           For now, don't allow calling get_input_ring in constructors
		     Just have users use output.set_space(lookup_string(params(), ...));

Bifrost pipeline flow ideas
---------------------------
Break flow up into discrete start/stop cycles
  This is batch-like, but is also fine for long/continuous-running jobs (observations)
  Flow begins with metadata `headers' from inputs
    ** How to avoid deadlock with cyclic inputs?
	** How to handle EOD signal propagating down pipeline at a specific 'time'?
	  Writers set an EOD byte offset in RingBuffer, and when a reader reads
	    to or past this point it is told about the EOD and given only the valid data.
		More specifically, readers wait on open_read until data are available
		  or EOD is set within their requested span, in which case they return
		  the available data and indicate that EOD was reached.
		  ** Might be able to just return an EOD frame number, which can be
		       used by the reading task and passed on to the outputs.

RecvUDP
  payloads: {shape: payload_size_max}

header:
{
	"__pipeline__": {...complete pipeline definition...},
	"__history__":  [],
	"__current__": {
	  "__source_task__": "PacketCapture",
	  "shape": [2,3,4],
	  "nbit":        4,
	  "param3": "blah",
	  ...
	  }
}

multiple inputs multiple outputs?
  ** Don't worry about history for now, as it complicates things
       E.g., 3 in + 3 out => each output's history includes all 3 inputs?
	     => exponential growth
       Find a way to automate it later

__history__: [{__name__: "Task0", __class__: "std/RecvUDP", blah:"blah"},
			  {task1}, ...],

create_output(name) -> Pipeline::create_ring(name)
input.init(ring)
output.init(ring)

void main() {
	while( !this->shutdown_requested ) {
		init();
		  headers = input.read_header() for input in non-cyclic inputs
		  output.write_header(new_header);
		clear_outputs();
		open_inputs();
		while( !eod ) {
			process_inputs();
			advance_inputs();
		}
	}
}


in = open/advance_inputs
if( in.eod ) {
	outputs.set_eod(in.eod_frame);
}

while !shutdown:
    clear_outputs
    recv_input_headers
	process_input_headers
    send_output_headers
    open_inputs
    while !eod:
      process
      advance_inputs

TBF
---
Output packet format:                [12 chans, 256 stands, 2 pols, 2 cpx, 4 bits] = 6144 bytes
Pipeline:
  RecvUDP                           [144 chans,  16 stands, 2 pols, 2 cpx, 4 bits]
  Depacketize     [time, 16 roaches, 144 chans,  16 stands, 2 pols, 2 cpx, 4 bits]
  Reorder         [time,             144 chans, 256 stands, 2 pols, 2 cpx, 4 bits]
  TBFPacketize    [packet,            12 chans, 256 stands, 2 pols, 2 cpx, 4 bits] = 6144 bytes
QUESTION: Use a flag input to filter which packets are actually transmitted?
            E.g., TBFPacketize task receives TBF commands and generates a
			        bool for each packet (or simply size=0?).
SendUDP: params: packet_size_max, rate_limit [Gb/s per dest], destination (optional)
         inputs: packets, headers, sizes, destinations (optional)
** TODO: sendmmsg can actually gather each packet from multiple source arrays,
           which means the headers could be stored separately to the payloads!
		   TBFPacketize would then just have to generate header and size streams
TBFPacketize:
  Wait for TBF command (doing nothing else)
  Open (large) input buffer at requested time window, with guarantee
  For each packet:
    Generate and write header to header output
	Copy packet data to data output
	Write size and destination to corresponding outputs
	  (Allow one destination per packet-subband; i.e., 12 destinations)

Reorder
-------
Depacketize     [time, 16 roaches, 144 chans,  16 stands, 2 pols, 2 cpx, 4 bits]
Reorder         [time,             144 chans, 256 stands, 2 pols, 2 cpx, 4 bits]
Transpose using elements of size 32 bytes

struct __attribute__((aligned(32))) aligned256_type {
  char data[32];
};

for( int chan=0; chan<nchan; ++chan ) {
  //for( int roach=0; roach<NROACH; ++roach ) {
#define COPY_CHUNK(roach,chan) \
  out[roach + NROACH*chan] = in[chan + nchan*roach];
    COPY_CHUNK( 0,chan); COPY_CHUNK( 1,chan);
	COPY_CHUNK( 2,chan); COPY_CHUNK( 3,chan);
	COPY_CHUNK( 4,chan); COPY_CHUNK( 5,chan);
	COPY_CHUNK( 6,chan); COPY_CHUNK( 7,chan);
	COPY_CHUNK( 8,chan); COPY_CHUNK( 9,chan);
	COPY_CHUNK(10,chan); COPY_CHUNK(11,chan);
	COPY_CHUNK(12,chan); COPY_CHUNK(13,chan);
	COPY_CHUNK(14,chan); COPY_CHUNK(15,chan);
  //}
}

Data flow start/stop
--------------------
Depacketize receives reset (or stop+start) command for future slot
  Command contains a new data header with UTC_START + other metadata

Live data monitoring
--------------------
// Broadcast data via ZMQ PUB socket
void publish(stream_name, const T* data, frame_shape, nframe, t0, dt, axis_names, axis_scales);

Correlator output rate
----------------------
39.2 MHz = 1568 channels
256*257/2 baselines * 4 pols * 8 bytes/sample = 1052672 bytes/channel = 1.651 GB/integration
Beamformer output rate is: 39.2 MHz * 2 pols * 2 bytes/sample = 156.8 MB/s
 => Equivalent correlator rate with 10.53 s integrations

Beamformer data ordering
------------------------
** If can always compute 32 beams for all channels, things are straightforward
     The overall limits will then be a max of 32 pointings and 39.2 MHz
	   with a max total output data rate of 39.2 MHz.
	   E.g.,  1 pointing  @ 39.2 MHz
	          2 pointings @ 19.6 MHz
			  4 pointings @  9.8 MHz
			  2 @ 9.8 + 1 @ 19.6
//Btvbd = Wvipbd * Vtvip
Input:   [250 time][144 chans][256 stands][2 pols][2 cpx][fixed8.7]
Output:  [32 beams][144 chans][250 time  ][2 pols][2 cpx][float32]
Requant: [32 beams][144 chans][250 time  ][2 pols][2 cpx][fixed8.7]
//Output: [time][144 chans][ 32 beams ][2 pols][2 cpx][float32]

Exchange beams for channels, with packets containing a sub-band and some time
  Send each beam to a different destination
  Receive each sub-band from a different source

Bbdvt = Wbdvip * Vtvip^ // This is also the most efficient ordering for gemm
Beamform ...
Send: [12 beamsets][<=3 beams][2 pols]             [144 chans][250 time][2 cpx][fixed8.7]
Recv:              [<=3 beams][2 pols][12 subbands][144 chans][250 time][2 cpx][fixed8.7]
Reintprt:          [<=3 beams][2 pols]            [1728 chans][250 time][2 cpx][fixed8.7]
Reorder:           [<=3 beams][2 pols]            [250 time][1728 chans][2 cpx][fixed8.7]
T-engine ...

T-engine input:    [beam][pol][time][144 chans][2 cpx][float32]
Merge subbands:    [beam][pol][time][all chans][2 cpx][float32]
# Note: Each beam will do its own freq->time conversion depending on its tuning
          Operations will be homogeneously batched over the [time] dimension though
freq->time output: [beam][pol][time           ][2 cpx][float32]
Requantize:        [beam][pol][time           ][2 cpx][sint8]

At 39.2 MHz, 40us = 1568 complex samples
Need 4096 samples per packet
So buffering up 4096 spectra allows creating 1568x 39.2   MHz packets
                                              784x 19.6   MHz packets
											  392x  9.8   MHz packets
											  196x  4.9   MHz packets
											   98x  2.45  MHz packets
											   49x  1.225 MHz packets

TBN IO constraints
------------------
1.0MHz => 512.0 MB/s total => 43 MB/s/stream
1.6MHz => 819.2 MB/s total => 69 MB/s/stream
ledaovro1~$ dd if=/dev/zero of=/data1/one/output conv=fdatasync bs=384k count=1k && rm -f /data1/one/output
> 104 MB/s
ledaovro12~$ dd if=/dev/zero of=/data1/one/output conv=fdatasync bs=384k count=1k && rm -f /data1/one/output
> 52.4 MB/s
adp@adp1:~$ dd if=/dev/zero of=/data/write_test.bin conv=fdatasync bs=384k count=2k && rm -f /data/write_test.bin
> 160 MB/s

96 chans/subband * 0.025 MHz/chan = 2.4 MHz (= 2 MHz + 10% guard bands)
2.4 MHz * 32 inputs * 4+4 bits    = 76.8 MB/s/subarray

Proposal: Write F-domain data directly to disk and do T-engine offline?
            Would allow writing 4+4 instead of 8+8 bits, doubling the BW
			Would allow easy and high-quality T-engine implem in Python
			Bifrost pipeline would just be: RecvUDP, ?Depacketize, ?FileWrite
			Required metadata is just stand0, nstand, chan0, nchan, and time information
			Data order on disk would be: [time, 96 chans, 16 stands, 2 pols, 2 cpx, 4 bits]

No. channels per packet/stream
------------------------------
   144 channels per stream (2 per server)
= 1728 total channels
= 43.2 MHz bandwidth
= 14.7456 Gbps per capture stream (2 per server, 12 total)

Correlator output packets
-------------------------
144 channels per stream
Each packet: [144 chans][2 pol][2 pol][2 complex + 1 weight + 1 bit padding][21 bits] = 4608 bytes

MSB                               LSB
0        8        16       24     31
======== ======== ======== ========
<------- --REAL-- ----><-- --------
IMAG---- -><----- -WEIGHT- ------>0
======== ======== ======== ========

Correlator processing:
  GainCorrect output: (time, 144 chan,   256 stand, 2 pol, 2 cpx, 8b)
  Correlator  output: (snap, 32896 bline, 144 chan, 2 pol, 2pol, 2 cpx, 21b)
                       + 22b weight
    Preproc:  Dequantize 8b->fp32
	Process:  cublasCherk (tcip,tcjq->cipjq) fp32
	Postproc: Reorder (cipjq->bcpq) and quantize fp32->21b + 22b weight
	(*Actually this postproc is really CorPacketize, which can then be sent directly
	    through a socket.)
	  Note: Exposing the raw output (cipjq, fp32) to other tasks would be very useful
	          as this is a natural ordering in which to apply calibration operations.

COR commands
------------
Applied at a target subslot
  The current integration should be pushed out (possibly with lower weights)
    and a new integration started.
Updates COR_NAVG (no. subslots to integrate) and COR_GAIN (alpha scale factor)
Updates DRX_TUNING_MASK
  This just means which 144-chan subbands are exported via the output socket and
    which are not needed.
	Could either always compute everything and then just filter packets, or
	  could try to disable computation altogether.
	  The STP command will work similarly (effectively equivalent to setting
	    DRX_TUNING_MASK to 0).
	  Note that the COR command technically only applies to the export of
	    correlator data, not to the generation of it.
		I.e., other tasks may want to use the correlator output even if
		  it's not being exported to storage.
		  E.g., RT calibration, imaging etc.
		This also means that the COR_NAVG and COR_GAIN parameters should
		  really be applied _after_ correlation.
		  E.g., GainCorrect->Correlate->Integrate(COR_NAVG,COR_GAIN)->CorPacketize(DRX_TUNING_MASK)
		  *This makes things easy wrt tracking time, as everything is continuous until the very last stage

Required modes
--------------
1) TBN: Continuous recording of 1 MHz from all dipoles using 8+8 bits
           1.0 MHz, 256 antennas,     8+8 bits, Nyquist
2) BX1: Continuous recording of 1 beam over 19.6 MHz and visibilities over 9.8 MHz
          19.6 MHz, 1 beam,       16+16 bits, Nyquist
          13.0 MHz, visibilities, 32+32 bits, 8 secs
3) BX2: Continuous recording of 2 beams over 13.0 MHz and visibilities over 13.0 MHz
          13.0 MHz, 2 beams,      16+16 bits, Nyquist
          13.0 MHz, visibilities, 32+32 bits, 8 secs

T-engine
--------
Should include some additional 'guard-band' channels either side of the target band
  E.g., 28 MHz (1120 chans) for 26 MHz (1040 chans) target
Multiply by bandpass window (Kaiser?) to spectrum before inverse Fourier transforming
  to the time domain.
Multiply by phase ramp to shift center frequency
Resample via FFT, crop, IFFT with window size a multiple of 14=28./26.*13
TODO: How big should the resampling FFT be?
      What needs to be done regarding overlap-save?
	  Does padding need to be included anywhere?
A smooth bandpass => compact time-domain filter
A small  delay    => compact time-domain filter



Oct 14 - Dec 14
Jan 15 - March 15
Substantive activity on which you focused
Oct-Dec 2014:
  Clean-up of pipeline management scripts
  Reordering of inputs before correlation
  New output format with triangular ordering and separate 1s outrigger visibilities
  Investigation and implementation of HDF5 conversion and BDI
Jan-Mar 2015:
  Aggregation of OVRO telescope metadata
  Streaming to transient cluster
  Dynamic control over disk writing
  Compression of visibility data
Apr-Jun 2015:
  Quick-look imaging scripts with visibility calibration
  Preliminary investigation of clean beamforming concept
  PeXC, Sevietta
Jul-Sep 2015:
  Integration of Sevietta codebase at OVRO
Distractions:
  Unreliability of state switching forcing rethink of data management
  Transient cluster streaming requirements conflicting with LEDA SOP
  Oct: LEDA
  Nov: LEDA
  Jan: LEDA, xGPU
  Feb: LEDA
  Mar: LEDA, PeXC
  Apr: PeXC
  May: PeXC, Sevietta
  Jun: Sevietta
  Jul: Sevietta


Packet capture/transmission
---------------------------
Need  block-based UDP capture and transmission for streaming data
Need packet-based UDP (or perhaps TCP) capture and transmission for monitor and control

Unpacker
--------
    [seq][32 packets][~2 time][88 chans]            [8 ants][2 pols][4+4 bits]
--> [seq]            [ 2 time][88 chans][32 packets][8 ants][2 pols][8+8 bits]
=> Transposing blocks of 8*2*2 bytes = 32 bytes

    [seq][16 packets][~2 time][144 chans]            [16 ants][2 pols][4+4 bits]
--> [seq]            [ 2 time][144 chans][16 packets][16 ants][2 pols][8+8 bits]
=> Transposing blocks of 16*2*1 bytes = 32 bytes

//uint32_t a = in[:32];
//uint32_t b = in[:32];
//uint32_t c = 
//uint32_t d =

System setup
------------
Software pipelines run as a service on the servers
  
Roach firmware controlled as a service?
  Log the UTC_START to a globally-shared space (nfs disk?) when starting
Hub service(?) connects to server pipelines

Pipeline
--------
RecvUDP
ReorderFrames
ADP::Unpack
ADP::GainCorrect
ADP::Correlate, ADP::Beamform, CopyD2H
ADP::PktzCorr,  ADP::PktzBeam, ADP::TBW (large input ring and guaranteed reads)
SendUDP         SendUDP        CnrTurnSend

// Services:
// adp-capture:    RecvUDP, ReorderFrames, Unpack, GainCorrect
// adp-beamformer: ADP::Beamform, SendUDP
// adp-correlator: ADP::Correlate, SendUDP
// adp-tbw:        CopyD2H, ADP::TBW, SendUDP

Standard tasks to implement:
  FIR filter
  FFT
  Filterbank (FIR, FFT)
  Coherent dedispersion (FIR)
  Transform
  Requantize (Transform)
  xGPU
  
Control commands
----------------
Common concepts:
  Stands are numbered 1-260
  Inputs are numbered 1-520, interleaved X,Y (so x1 y1 x2 y2 ... x260 y260)
    Note: Inputs are referred to as 'channels' in the DP ICD docs
----------------
FST: Configure FIR coefs
  sint16 INDEX: -1 => Load defaults from file
                 0 => Apply to all inputs
                 i => Apply to input i
  sint16 COEFF_DATA[16][32]: filters for each of 16 fine delays
  Internal ADP data format: complex64 weights[nchan][nstand][pol];
  Max delay is 28 @ 196 MHz

BAM:
  Max delay is 1023+15/16 = 1023.9375 @ 196 MHz = 5.224170918 us = 1 / 7.65671733 spectra
    ** Redefine limit to 256 @ 196 MHz = 1.30612245 us = 1 / 30.625 spectra
         (Or check the max dynamically, if have time to implement)

DRX: 1x 39.2   MHz = 196 MHz /   5 = 1568x 25 kHz
     2x 19.6   MHz = 196 MHz /  10 =  784x 25 kHz
     4x  9.8   MHz = 196 MHz /  20 =  392x 25 kHz
     8x  4.9   MHz = 196 MHz /  40 =  196x 25 kHz
    16x  2.45  MHz = 196 MHz /  80 =   98x 25 kHz
    32x  1.225 MHz = 196 MHz / 160 =   49x 25 kHz
	Add guard bands of 4x 0.025 to either side of each sub-band
      => Max extra of 32x 0.2 MHz = 6.4 MHz
      => Max total of 39.2 MHz + 6.4 MHz = 45.6 MHz (~= 78% of peak 40 GbE)

TBN: Stops any active beams and correlator, waits for any active TBW to finish
DRX: Set tunings for TBW/beam/corr
BAM: [Start and] configure the beamformer
TBW: Trigger a TBW dump of the current tunings
Question: How to allow TBW dump while in TBN mode?

  
